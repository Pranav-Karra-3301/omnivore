<p align="center">
  <img src="https://ov.pranavkarra.me/logo.png" alt="ov" width="200"/>
</p>


[![Rust](https://img.shields.io/badge/rust-1.88%2B-orange.svg)](https://www.rust-lang.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub release](https://img.shields.io/github/v/release/Pranav-Karra-3301/omnivore)](https://github.com/Pranav-Karra-3301/omnivore/releases)

# Omnivore

**Universal Rust Web Crawler & Knowledge Graph Builder**

A high-performance, parallel web crawler and knowledge graph system built in Rust, designed for scale and intelligence.

## Features

- **Parallel Crawling**: Async/await with Tokio, supporting 1000+ concurrent connections
- **Dual-Mode Operation**: Static (Reqwest) and dynamic (browser automation) crawling
- **Smart Content Extraction**: CSS selectors, XPath, and pattern matching
- **Knowledge Graph**: Build and query entity-relationship graphs from crawled data
- **Politeness Engine**: Per-domain rate limiting with exponential backoff
- **Extensible Architecture**: Plugin system for custom extractors and processors

## Quick Start

### Installation

#### Homebrew (macOS/Linux)
```bash
# Install from tap (once published)
brew tap Pranav-Karra-3301/omnivore
brew install omnivore

# Or install from source
git clone https://github.com/Pranav-Karra-3301/omnivore.git
cd omnivore
brew install --build-from-source ./Formula/omnivore.rb
```

#### From Source
```bash
# Clone the repository
git clone https://github.com/Pranav-Karra-3301/omnivore.git
cd omnivore

# Build the project
cargo build --release

# Install locally
make install
```

#### Docker
```bash
# Run with Docker
docker run -it --rm -p 3000:3000 omnivore:latest

# Or use docker-compose for full stack
docker-compose up -d
```

### Basic Usage

```bash
# Crawl a website
omnivore crawl https://example.com --workers 10 --depth 5

# Parse HTML with custom rules
omnivore parse index.html --rules parser-rules.yaml

# Build knowledge graph
omnivore graph crawl-results.json --output graph.db

# Start API server
cargo run --bin omnivore-api
```

## Architecture

```
omnivore/
├── omnivore-core/     # Core crawler and processing engine
├── omnivore-cli/      # Command-line interface
├── omnivore-api/      # REST and GraphQL API
└── configs/           # Configuration files
```

## Core Components

### Crawler Engine
- Tokio-based async runtime with configurable worker pools
- Automatic routing between static and dynamic crawlers
- URL frontier with priority queue and bloom filter deduplication
- Robots.txt compliance and politeness controls

### Parser System
- HTML parsing with Scraper (html5ever backend)
- CSS and XPath selectors
- Structured data extraction (JSON-LD, Microdata, OpenGraph)
- Custom extraction rules with regex patterns

### Intelligence Layer
- Entity recognition (emails, phones, dates, prices)
- Relationship extraction from unstructured text
- Content classification and tagging
- Vector embeddings for semantic search

### Knowledge Graph
- Petgraph for in-memory graph operations
- Schema-based validation
- Query engine with graph algorithms
- Export to various graph formats

## Configuration

Create a `crawler.toml` file:

```toml
[crawler]
max_workers = 100
max_depth = 10
user_agent = "Omnivore/1.0"
respect_robots_txt = true

[crawler.politeness]
default_delay_ms = 100
max_requests_per_second = 10.0

[parser]
clean_text = true
extract_metadata = true

[storage]
data_dir = "./data"
compression = "zstd"
```

## API Usage

### REST API

```bash
# Start crawl
curl -X POST http://localhost:3000/api/crawl \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "max_depth": 5}'

# Get statistics
curl http://localhost:3000/api/stats
```

### GraphQL

```graphql
query {
  health
  version
}
```

## Performance

- Process 10,000+ pages per minute on commodity hardware
- Sub-100ms latency for graph queries
- Memory-efficient frontier management (< 500MB for 1M URLs)
- Automatic scaling from 1 to 10,000 workers

## Development

### Building from Source

```bash
# Debug build
cargo build

# Release build with optimizations
cargo build --release

# Run tests
cargo test

# Run benchmarks
cargo bench
```

### Testing

```bash
# Unit tests
cargo test --lib

# Integration tests
cargo test --test '*'

# Specific test
cargo test test_crawler_creation
```

## Roadmap

- [ ] Distributed crawling with worker nodes
- [ ] Machine learning for content classification
- [ ] Browser fingerprint randomization
- [ ] WASM plugin support
- [ ] Kubernetes operator
- [ ] Real-time graph updates via WebSocket

## Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details.

## License

This project is dual-licensed under MIT OR Apache-2.0.

## Acknowledgments

Built with these amazing Rust crates:
- Tokio for async runtime
- Reqwest for HTTP client
- Scraper for HTML parsing
- Petgraph for graph algorithms
- Axum for web framework

---

**Note**: This is a production-grade foundation that can be extended with additional features as needed.
